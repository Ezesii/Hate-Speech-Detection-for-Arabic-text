{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REFERENCE\n",
    "\n",
    "@inproceedings{inoue-etal-2021-interplay,\n",
    "    title = \"The Interplay of Variant, Size, and Task Type in {A}rabic Pre-trained Language Models\",\n",
    "    author = \"Inoue, Go  and\n",
    "      Alhafni, Bashar  and\n",
    "      Baimukan, Nurpeiis  and\n",
    "      Bouamor, Houda  and\n",
    "      Habash, Nizar\",\n",
    "    booktitle = \"Proceedings of the Sixth Arabic Natural Language Processing Workshop\",\n",
    "    month = apr,\n",
    "    year = \"2021\",\n",
    "    address = \"Kyiv, Ukraine (Online)\",\n",
    "    publisher = \"Association for Computational Linguistics\",\n",
    "    abstract = \"In this paper, we explore the effects of language variants, data sizes, and fine-tuning task types in Arabic pre-trained language models. To do so, we build three pre-trained language models across three variants of Arabic: Modern Standard Arabic (MSA), dialectal Arabic, and classical Arabic, in addition to a fourth language model which is pre-trained on a mix of the three. We also examine the importance of pre-training data size by building additional models that are pre-trained on a scaled-down set of the MSA variant. We compare our different models to each other, as well as to eight publicly available models by fine-tuning them on five NLP tasks spanning 12 datasets. Our results suggest that the variant proximity of pre-training data to fine-tuning data is more important than the pre-training data size. We exploit this insight in defining an optimized system selection model for the studied tasks.\",\n",
    "}\n",
    "\n",
    "Khalid, S. (2020) Bert explained: A Complete Guide with Theory and tutorial, Medium. Available at: https://medium.com/@samia.khalid/bert-explained-a-complete-guide-with-theory-and-tutorial-3ac9ebc8fa7c (Accessed: 17 September 2023). \n",
    "\n",
    "Classify text with Bert&nbsp; :&nbsp;  text&nbsp; :&nbsp;  tensorflow (no date) TensorFlow. Available at: https://www.tensorflow.org/text/tutorials/classify_text_with_bert (Accessed: 17 September 2023). \n",
    "\n",
    "Text classification using Bert &amp; Tensorflow | Deep Learning Tutorial 47 (Tensorflow, Keras &amp; Python) (2021) YouTube. Available at: https://www.youtube.com/watch?v=hOCDJyZ6quA (Accessed: 17 September 2023). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-t0hKnN9-TXG"
   },
   "source": [
    "#### CAMeLBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LClFcGjk-Vua",
    "outputId": "3c901733-dbf2-42d8-e628-6f55afc08c7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.32.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n",
      "Requirement already satisfied: torch!=1.12.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.0.1+cu118)\n",
      "Requirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.22.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers[torch]) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers[torch]) (4.7.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (3.27.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (16.0.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.9->transformers[torch]) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.9->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.22.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "/usr/local/lib/python3.10/dist-packages/accelerate/__init__.py\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/__init__.py\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers[torch]\n",
    "!pip install accelerate -U\n",
    "import accelerate\n",
    "import transformers\n",
    "print(accelerate.__file__)\n",
    "print(transformers.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "eaDK1M-6-W0H"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('UseThisClean.csv')\n",
    "comments = df['clean_text'].tolist()\n",
    "labels = df['Class'].tolist()\n",
    "\n",
    "# Split data: 80% training, 20% validation\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(comments, labels, test_size=0.2, random_state=42, stratify=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "65V5KJrO-gRl",
    "outputId": "22995def-3973-4784-a0d6-222dc7762095"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-mix and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('CAMeL-Lab/bert-base-arabic-camelbert-mix')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('CAMeL-Lab/bert-base-arabic-camelbert-mix', num_labels=3) # 3 classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "bI5wETOL-mf4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load your specific tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('CAMeL-Lab/bert-base-arabic-camelbert-mix')\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('UseThisClean.csv')\n",
    "\n",
    "# Drop NaN values\n",
    "data = data.dropna(subset=['clean_text'])\n",
    "\n",
    "# Convert the 'clean_text' column to string\n",
    "data['clean_text'] = data['clean_text'].astype(str)\n",
    "\n",
    "# Filter out empty strings\n",
    "data = data[data['clean_text'].str.strip() != \"\"]\n",
    "\n",
    "# Extract the 'clean_text' column and the 'Class' column\n",
    "texts = data['clean_text'].tolist()\n",
    "labels = data['Class'].tolist()\n",
    "\n",
    "# Tokenize the data\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, max_length=25, return_tensors='pt')\n",
    "\n",
    "# Extract input_ids and attention masks\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_masks = inputs[\"attention_mask\"]\n",
    "\n",
    "# Convert labels to tensors\n",
    "label_tensors = torch.tensor(labels)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n",
    "    input_ids, attention_masks, label_tensors, test_size=0.2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "73Yb3P9l-uj3",
    "outputId": "2fd98fe1-7d17-4787-8ff9-25247c3ce06a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2])\n",
      "torch.int64\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import torch\n",
    "\n",
    "# Define the custom dataset\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'input_ids': self.inputs[idx], 'labels': self.labels[idx]}\n",
    "\n",
    "# Instantiate the datasets\n",
    "train_dataset = CustomDataset(train_inputs, train_labels)\n",
    "eval_dataset = CustomDataset(val_inputs, val_labels)\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "# Create a tensor with float32 data type\n",
    "labels = torch.tensor([0.0, 1.0, 2.0], dtype=torch.float32)\n",
    "\n",
    "# Convert to int64 (Long tensor)\n",
    "labels = labels.to(dtype=torch.int64)\n",
    "\n",
    "print(labels)\n",
    "print(labels.dtype)\n",
    "\n",
    "# Convert train_labels to int64 (Long tensor)\n",
    "train_labels = train_labels.to(dtype=torch.int64)\n",
    "\n",
    "\n",
    "\n",
    "# Now print the dtype to confirm\n",
    "print(train_labels.dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "jDxEWajB_7p_"
   },
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./work',  # <-- This line is added. Adjust the path as needed.\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    fp16=True,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=200,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    dataloader_pin_memory=False,  # Add this line to disable memory pinning\n",
    "    evaluation_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Define the metric computation\n",
    "def compute_metrics(p):\n",
    "    pred_labels = p.predictions.argmax(-1)\n",
    "    labels = p.label_ids\n",
    "    accuracy = accuracy_score(labels, pred_labels)\n",
    "    report = classification_report(labels, pred_labels)\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'classification_report': report\n",
    "    }\n",
    "\n",
    "# Move input tensors to the same device as the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_inputs = train_inputs.to(device)\n",
    "val_inputs = val_inputs.to(device)\n",
    "train_labels = train_labels.to(device)\n",
    "val_labels = val_labels.to(device)\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "train_inputs = train_inputs.to(device)\n",
    "val_inputs = val_inputs.to(device)\n",
    "train_labels = train_labels.to(device)\n",
    "val_labels = val_labels.to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 848
    },
    "id": "XjeGn6yWABHk",
    "outputId": "8a3ac532-f26f-4272-9aea-1966d6896bd4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='879' max='879' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [879/879 06:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Classification Report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.660100</td>\n",
       "      <td>0.513936</td>\n",
       "      <td>0.785836</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "           0       0.77      0.79      0.78       834\n",
       "           1       0.70      0.72      0.71       786\n",
       "           2       0.91      0.85      0.88       724\n",
       "\n",
       "    accuracy                           0.79      2344\n",
       "   macro avg       0.79      0.79      0.79      2344\n",
       "weighted avg       0.79      0.79      0.79      2344\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.457700</td>\n",
       "      <td>0.552561</td>\n",
       "      <td>0.781143</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "           0       0.79      0.73      0.76       834\n",
       "           1       0.69      0.74      0.71       786\n",
       "           2       0.87      0.88      0.88       724\n",
       "\n",
       "    accuracy                           0.78      2344\n",
       "   macro avg       0.79      0.78      0.78      2344\n",
       "weighted avg       0.78      0.78      0.78      2344\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.151200</td>\n",
       "      <td>0.712607</td>\n",
       "      <td>0.787116</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "           0       0.78      0.78      0.78       834\n",
       "           1       0.71      0.70      0.71       786\n",
       "           2       0.88      0.89      0.88       724\n",
       "\n",
       "    accuracy                           0.79      2344\n",
       "   macro avg       0.79      0.79      0.79      2344\n",
       "weighted avg       0.79      0.79      0.79      2344\n",
       "</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.79      0.78       834\n",
      "           1       0.70      0.72      0.71       786\n",
      "           2       0.91      0.85      0.88       724\n",
      "\n",
      "    accuracy                           0.79      2344\n",
      "   macro avg       0.79      0.79      0.79      2344\n",
      "weighted avg       0.79      0.79      0.79      2344\n",
      "\" of type <class 'str'> for key \"eval/classification_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.73      0.76       834\n",
      "           1       0.69      0.74      0.71       786\n",
      "           2       0.87      0.88      0.88       724\n",
      "\n",
      "    accuracy                           0.78      2344\n",
      "   macro avg       0.79      0.78      0.78      2344\n",
      "weighted avg       0.78      0.78      0.78      2344\n",
      "\" of type <class 'str'> for key \"eval/classification_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.78      0.78       834\n",
      "           1       0.71      0.70      0.71       786\n",
      "           2       0.88      0.89      0.88       724\n",
      "\n",
      "    accuracy                           0.79      2344\n",
      "   macro avg       0.79      0.79      0.79      2344\n",
      "weighted avg       0.79      0.79      0.79      2344\n",
      "\" of type <class 'str'> for key \"eval/classification_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=879, training_loss=0.37661943012536997, metrics={'train_runtime': 392.993, 'train_samples_per_second': 71.566, 'train_steps_per_second': 2.237, 'total_flos': 361331292656250.0, 'train_loss': 0.37661943012536997, 'epoch': 3.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "id": "iGpeZY51E644",
    "outputId": "2f172a04-b3f6-425b-afbc-e6240fc71aee"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='586' max='586' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [586/586 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.78      0.78       834\n",
      "           1       0.71      0.70      0.71       786\n",
      "           2       0.88      0.89      0.88       724\n",
      "\n",
      "    accuracy                           0.79      2344\n",
      "   macro avg       0.79      0.79      0.79      2344\n",
      "weighted avg       0.79      0.79      0.79      2344\n",
      "\" of type <class 'str'> for key \"eval/classification_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7126073241233826, 'eval_accuracy': 0.7871160409556314, 'eval_classification_report': '              precision    recall  f1-score   support\\n\\n           0       0.78      0.78      0.78       834\\n           1       0.71      0.70      0.71       786\\n           2       0.88      0.89      0.88       724\\n\\n    accuracy                           0.79      2344\\n   macro avg       0.79      0.79      0.79      2344\\nweighted avg       0.79      0.79      0.79      2344\\n', 'eval_runtime': 9.8104, 'eval_samples_per_second': 238.93, 'eval_steps_per_second': 59.732, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate()\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "h5hlOm0WGQnk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
